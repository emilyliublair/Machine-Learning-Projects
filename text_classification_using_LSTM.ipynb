{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_using_LSTM.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilyliublair/Machine-Learning-Projects/blob/main/text_classification_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "source": [
        "#load, split, encode data\n",
        "train_dataset = pd.read_csv(train_file_path, sep='\\t')\n",
        "test_dataset = pd.read_csv(test_file_path, sep='\\t')\n",
        "\n",
        "train_dataset.loc[-1] = [train_dataset.columns[0], train_dataset.columns[1]]\n",
        "train_dataset.index = train_dataset.index + 1\n",
        "train_dataset.sort_index(inplace=True)\n",
        "test_dataset.loc[-1] = [test_dataset.columns[0], test_dataset.columns[1]]\n",
        "test_dataset.index = test_dataset.index + 1\n",
        "test_dataset.sort_index(inplace=True)\n",
        "\n",
        "train_dataset = train_dataset.rename(columns={train_dataset.columns[0]:'indicator', train_dataset.columns[1]:'text'})\n",
        "test_dataset = test_dataset.rename(columns={test_dataset.columns[0]:'indicator', test_dataset.columns[1]:'text'})\n",
        "train_eval = train_dataset.pop('indicator')\n",
        "test_eval = test_dataset.pop('indicator')\n",
        "\n",
        "train_eval = train_eval.replace({'ham':0})\n",
        "train_eval = train_eval.replace({'spam':1})\n",
        "test_eval = test_eval.replace({'ham':0})\n",
        "test_eval = test_eval.replace({'spam':1})\n",
        "\n",
        "vocab = {}  \n",
        "word_encoding = 1\n",
        "def one_hot_encoding(text):\n",
        "  global word_encoding\n",
        "\n",
        "  words = text.lower().split(\" \") \n",
        "  encoding = []  \n",
        "\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      code = vocab[word]  \n",
        "      encoding.append(code) \n",
        "    else:\n",
        "      vocab[word] = word_encoding\n",
        "      encoding.append(word_encoding)\n",
        "      word_encoding += 1\n",
        "  \n",
        "  return encoding\n",
        "\n",
        "for x in range(0, train_dataset.shape[0]):\n",
        "  encoding = one_hot_encoding(train_dataset.loc[x]['text'])\n",
        "\n",
        "for x in range(0, test_dataset.shape[0]):\n",
        "  encoding = one_hot_encoding(test_dataset.loc[x]['text'])\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "for x in range(0, train_dataset.shape[0]):\n",
        "  i = train_dataset.loc[x]['text'].split(\" \")\n",
        "  num = 0\n",
        "  list1 = []\n",
        "  for y in i:\n",
        "    i[num] = (vocab[y])\n",
        "    list1.append(i[num])\n",
        "    num+=1\n",
        "  train_dataset.loc[x]['text'] = list1\n",
        "\n",
        "for x in range(0, test_dataset.shape[0]):\n",
        "  i = test_dataset.loc[x]['text'].split(\" \")\n",
        "  num = 0\n",
        "  list1 = []\n",
        "  for y in i:\n",
        "    i[num] = (vocab[y])\n",
        "    list1.append(i[num])\n",
        "    num+=1\n",
        "  test_dataset.loc[x]['text'] = list1\n",
        "\n",
        "train_dataset = train_dataset.values.tolist()\n",
        "num1=0\n",
        "for x in train_dataset:\n",
        "  y = train_dataset[num1][0]\n",
        "  train_dataset[num1]= y\n",
        "  num1+=1\n",
        "\n",
        "test_dataset = test_dataset.values.tolist()\n",
        "num2=0\n",
        "for x in test_dataset:\n",
        "  y = test_dataset[num2][0]\n",
        "  test_dataset[num2]= y\n",
        "  num2+=1\n",
        "\n",
        "train_dataset = np.array(train_dataset)\n",
        "test_dataset = np.array(test_dataset)\n",
        "train_eval = np.array(train_eval)\n",
        "test_eval = np.array(test_eval)\n",
        "print(train_dataset)\n",
        "print(test_dataset)\n",
        "print(train_eval)\n",
        "print(test_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "source": [
        "#preprocess data\n",
        "VOCAB_SIZE = len(vocab)\n",
        "train_dataset = keras.preprocessing.sequence.pad_sequences(train_dataset, 250)\n",
        "test_dataset = keras.preprocessing.sequence.pad_sequences(test_dataset, 250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URaTpZWBzG0j"
      },
      "source": [
        "#create model\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(VOCAB_SIZE, 32), tf.keras.layers.LSTM(32), tf.keras.layers.Dense(1, activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heHskQbj0D7q"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA_twA7q0GiE"
      },
      "source": [
        "#train model\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['acc'])\n",
        "history = model.fit(train_dataset, train_eval, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew2ZWcNN1Qn7"
      },
      "source": [
        "#function for encoding text\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [vocab[word] if word in vocab else 0 for word in tokens]\n",
        "  return keras.preprocessing.sequence.pad_sequences([tokens], 250)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9Zv4zlG17Zv"
      },
      "source": [
        "#function for decoding numbers\n",
        "reverse_vocab = {value:key for (key, value) in vocab.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text=\"\"\n",
        "  for num in integers:\n",
        "    if num!=PAD:\n",
        "      text+=reverse_vocab[num] +\" \"\n",
        "  return text[:-1]\n",
        "\n",
        "print(decode_integers(encoded))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "  encoded_text = encode_text(pred_text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  decision = \"\"\n",
        "  prediction = result[0]\n",
        "  if prediction <= .5:\n",
        "    decision = \"ham\"\n",
        "  else:\n",
        "    decision = \"spam\"\n",
        "\n",
        "  prediction = np.append(prediction, decision)\n",
        "  return (prediction)\n",
        "\n",
        "pred_text = \"wow, is your arm alright. that happened to me one time too\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}